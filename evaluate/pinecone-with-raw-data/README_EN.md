# Overall Evaluation of the Multiple-Choice Question Generation Chatbot Model for Project Management Using Raw Data Stored in Pinecone.
## ‚öôÔ∏è Set up

### Set up rouge_score

```bash
pip install rouge_score
```

## üßë‚Äçüíª Using

### Prepare the text files (TXT)

1. Generate questions using the three mode of our chatbot (ChatGPT, ClaudeAI, Gemini) with raw data stored in Pinecone.
2. Save these questions in the quizzes-txt directory as text files (TXT) corresponding to each model:
   - Questions from ChatGPT: `gpt_quizzes.txt`
   - Questions from ClaudeAI: `claude_quizzes.txt`
   - Questions from Gemini: `gemini_quizzes.txt`

### Prepare the JSON files

1. Run the `txt2json.py` file to convert data from TXT files to JSON files.
2. The JSON file will be saved in the `quizzes-json` directory with the corresponding model names:
   - The TXT file of ChatGPT (`gpt_quizzes.txt`) will be converted into `gpt_quizzes.json`
   - The TXT file of ClaudeAI (`claude_quizzes.txt`) will be converted into `claude_quizzes.json`
   - The TXT file of Gemini (`gemini_quizzes.txt`) will be converted into `gemini_quizzes.json`

### Measure ROUGE

1. Run the `metric.py` file to measure ROUGE for the questions generated by the 3 models:
   - ChatGPT (`gpt_quizzes.json`)
   - ClaudeAI (`claude_quizzes.json`)
   - Gemini (`gemini_quizzes.json`)

```bash
python metric.py
```

2. The results, as shown in the image below, will indicate the ROUGE scores for each set of questions generated by each model, helping you evaluate and compare the quality of the chatbot models in generating questions.  
![result](https://github.com/user-attachments/assets/5fe1cec1-fb9c-4caf-9237-9af15cf1e2d5)

